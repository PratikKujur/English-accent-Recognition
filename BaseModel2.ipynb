{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Custom Dataset to load the .npy files\n",
    "class AccentDataset(Dataset):\n",
    "    def __init__(self, feature_dir):\n",
    "        self.feature_dir = feature_dir\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Get all .npy files and their corresponding folder names (targets)\n",
    "        for folder in os.listdir(feature_dir):\n",
    "            folder_path = os.path.join(feature_dir, folder)\n",
    "            if os.path.isdir(folder_path):\n",
    "                for file in os.listdir(folder_path):\n",
    "                    if file.endswith('.npy'):\n",
    "                        self.files.append(os.path.join(folder_path, file))\n",
    "                        self.labels.append(folder)\n",
    "        \n",
    "        # Encode the folder names (categories) to numeric labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        features = np.load(file_path)  # Load the .npy file\n",
    "        features = torch.tensor(features).float()  # Convert to tensor\n",
    "        label = torch.tensor(self.labels[idx]).long()  # Get label\n",
    "        return features, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class SimpleConformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SimpleConformer, self).__init__()\n",
    "        self.conformer = nn.Sequential(\n",
    "            nn.Conv1d(768, 32, kernel_size=3, stride=1, padding=1),  # Example Conv layer\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.flattened_size = 64 * 162\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3),  # Output layer for 3 categories\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conformer(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "# Example usage\n",
    "model = SimpleConformer(208002,3)\n",
    "input_tensor = torch.randn(16, 768, 649)  # Batch of 16, 768 channels, sequence length 649\n",
    "output = model(input_tensor)\n",
    "print(output.shape) \n",
    "\n",
    "def compute_accuracy(preds, labels):\n",
    "    _, predicted = torch.max(preds, 1)  # Get the index of the max log-probability\n",
    "    correct = (predicted == labels).sum().item()  # Count correct predictions\n",
    "    accuracy = correct / labels.size(0)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0663, Accuracy: 0.4156\n",
      "Epoch 2/10, Loss: 0.0636, Accuracy: 0.4720\n",
      "Epoch 3/10, Loss: 0.0599, Accuracy: 0.5255\n",
      "Epoch 4/10, Loss: 0.0564, Accuracy: 0.5724\n",
      "Epoch 5/10, Loss: 0.0528, Accuracy: 0.6083\n",
      "Epoch 6/10, Loss: 0.0490, Accuracy: 0.6417\n",
      "Epoch 7/10, Loss: 0.0453, Accuracy: 0.6719\n",
      "Epoch 8/10, Loss: 0.0416, Accuracy: 0.7015\n",
      "Epoch 9/10, Loss: 0.0380, Accuracy: 0.7295\n",
      "Epoch 10/10, Loss: 0.0348, Accuracy: 0.7511\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "dataset = AccentDataset(feature_dir='TrialDataset')\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "input_dim = 208002  # Your input dimension (sequence length)\n",
    "num_classes = 3  # Number of accent categories\n",
    "model = SimpleConformer(input_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for features, labels in dataloader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        \n",
    "        \n",
    "        features = features.squeeze(1)\n",
    "        features = features.transpose(1, 2)  \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "            \n",
    "            # Compute accuracy\n",
    "        accuracy = compute_accuracy(outputs, labels)\n",
    "        correct_predictions += accuracy * labels.size(0)\n",
    "        total_predictions += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / total_predictions\n",
    "    epoch_accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        \n",
    "print('Training completed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'model_3cat')\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data and Perform Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 208002  # Your input dimension (sequence length)\n",
    "num_classes = 3  # Number of accent categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prathik\\AppData\\Local\\Temp\\ipykernel_1328\\3072476917.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_3cat'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleConformer(\n",
       "  (conformer): Sequential(\n",
       "    (0): Conv1d(768, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=10368, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model for testing/evaluation\n",
    "model = SimpleConformer(input_dim, num_classes)\n",
    "model.load_state_dict(torch.load('model_3cat'))\n",
    "model.eval()  # Set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccentDataset(Dataset):\n",
    "    def __init__(self, feature_dir):\n",
    "        self.feature_dir = feature_dir\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Get all .npy files and their corresponding folder names (targets)\n",
    "        for folder in os.listdir(feature_dir):\n",
    "            folder_path = os.path.join(feature_dir, folder)\n",
    "            if os.path.isdir(folder_path):\n",
    "                for file in os.listdir(folder_path):\n",
    "                    if file.endswith('.npy'):\n",
    "                        self.files.append(os.path.join(folder_path, file))\n",
    "                        self.labels.append(folder)\n",
    "        \n",
    "        # Encode the folder names (categories) to numeric labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        features = np.load(file_path)  # Load the .npy file\n",
    "        features = torch.tensor(features).float()  # Convert to tensor\n",
    "        features = features.squeeze(0)  # Add channel dimension if needed\n",
    "        features = features.permute(1, 0)\n",
    "        label = torch.tensor(self.labels[idx]).long()  # Get label\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    return accuracy, report, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prathik\\AppData\\Local\\Temp\\ipykernel_1328\\588248284.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_3cat'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.64%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.74      0.78     10047\n",
      "           1       0.75      0.84      0.79      9486\n",
      "           2       0.83      0.81      0.82      7000\n",
      "\n",
      "    accuracy                           0.80     26533\n",
      "   macro avg       0.80      0.80      0.80     26533\n",
      "weighted avg       0.80      0.80      0.80     26533\n",
      "\n",
      "Confusion Matrix:\n",
      "[[7466 1893  688]\n",
      " [1018 7969  499]\n",
      " [ 542  763 5695]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7963667885274941,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.83      0.74      0.78     10047\\n           1       0.75      0.84      0.79      9486\\n           2       0.83      0.81      0.82      7000\\n\\n    accuracy                           0.80     26533\\n   macro avg       0.80      0.80      0.80     26533\\nweighted avg       0.80      0.80      0.80     26533\\n',\n",
       " array([[7466, 1893,  688],\n",
       "        [1018, 7969,  499],\n",
       "        [ 542,  763, 5695]], dtype=int64))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dir = 'TrialDataset'\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = AccentDataset(test_dir)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load your model (replace 'YourModelClass' with the actual class of your model)\n",
    "model = SimpleConformer(input_dim, num_classes)\n",
    "model.load_state_dict(torch.load('model_3cat'))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
